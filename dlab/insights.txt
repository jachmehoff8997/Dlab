7.activation functions
import numpy as np
import matplotlib.pyplot as plt

# Activation functions
sigmoid = lambda x: 1 / (1 + np.exp(-x))
tanh = np.tanh
relu = lambda x: np.maximum(0, x)
leaky_relu = lambda x, alpha=0.01: np.where(x > 0, x, alpha * x)
softmax = lambda x: np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x)))

# Plotting function
def plot_activation_function(activation_function, name):
    x = np.linspace(-5, 5, 100)
    plt.plot(x, activation_function(x), label=name)
    plt.title(f"{name} Activation Function")
    plt.xlabel("Input")
    plt.ylabel("Output")
    plt.legend()
    plt.grid(True)
    plt.show()

# Plotting activation functions
plot_activation_function(sigmoid, "Sigmoid")
plot_activation_function(tanh, "Tanh")
plot_activation_function(relu, "ReLU")
plot_activation_function(lambda x: leaky_relu(x, 0.01), "Leaky ReLU (alpha=0.01)")

# Example usage of softmax
softmax_input = np.array([3.0, 1.0, 0.2])
softmax_output = softmax(softmax_input)
print("Softmax output:", softmax_output)
